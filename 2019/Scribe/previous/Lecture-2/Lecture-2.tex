\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\usepackage{graphicx}
\usepackage{url}

%
% The following commands sets up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}
\newcommand{\dnl}{\mbox{}\par}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
  \pagestyle{myheadings}
  \thispagestyle{plain}
  \newpage
  \setcounter{lecnum}{#1}
  \setcounter{page}{1}
  \noindent
  \begin{center}
  \framebox{
     \vbox{\vspace{2mm}
   \hbox to 6.28in { {\bf CMPSCI~630~~~Systems
                       \hfill Fall 2014} }
      \vspace{4mm}
      \hbox to 6.28in { {\Large \hfill Lecture #1  \hfill} }
%       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
      \vspace{2mm}
      \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
     \vspace{2mm}}
  }
  \end{center}
  \markboth{Lecture #1: #2}{Lecture #1: #2}
  \vspace*{4mm}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
%
\renewcommand{\cite}[1]{[#1]}

% \input{epsf}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{FIGURE-SIZE}{CAPTION}{FILENAME}
\newcommand{\fig}[4]{
           \vspace{0.2 in}
           \setlength{\epsfxsize}{#2}
           \centerline{\epsfbox{#4}}
           \begin{center}
           Figure \thelecnum.#1:~#3
           \end{center}
   }

% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% Some useful equation alignment commands, borrowed from TeX
\makeatletter
\def\eqalign#1{\,\vcenter{\openup\jot\m@th
 \ialign{\strut\hfil$\displaystyle{##}$&$\displaystyle{{}##}$\hfil
     \crcr#1\crcr}}\,}
\def\eqalignno#1{\displ@y \tabskip\@centering
 \halign to\displaywidth{\hfil$\displaystyle{##}$\tabskip\z@skip
   &$\displaystyle{{}##}$\hfil\tabskip\@centering
   &\llap{$##$}\tabskip\z@skip\crcr
   #1\crcr}}
\def\leqalignno#1{\displ@y \tabskip\@centering
 \halign to\displaywidth{\hfil$\displaystyle{##}$\tabskip\z@skip
   &$\displaystyle{{}##}$\hfil\tabskip\@centering
   &\kern-\displaywidth\rlap{$##$}\tabskip\displaywidth\crcr
   #1\crcr}}
\makeatother

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:



% Some general latex examples and examples making use of the
% macros follow.

\begin{document}

%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{2}{September 4}{Emery Berger}{Theodore Sudol, Amee Trivedi}

\section{Advice on Reviews}

Reviews from other semesters are available on HotCRP. This can help guide in
writing your own reviews, along with pointing out facets of the paper that you
may have missed. The summaries should be high-level syntheses, not simple
summations. An understanding of the contributions of and concepts within the
paper should be shown.

Keep in mind that, for many CS papers, a problem is solved with a novel
solution, but the former is forgotten while the latter lives on.

A useful guideline is George Heilmeier's ``catechism'' or criteria for research
projects. The relevant questions for our reviews include:
\begin{itemize}
  \item What are you trying to do? Use no jargon.
  \item How is it done today? What are the limitations of current practice?
  \item What's new in your approach? Why do you think it will be successful?
  \item Who cares?
  \item What are the risks and payoffs?
  \item How long will it take? What are the milestones?
\end{itemize}

\section{Compilation and Optimization}

Remember that optimization should not change the semantics of a program. If you
have some program \emph{P} that can be optimized into program $P'$, then
$\forall x : P(x) = P'(x)$.

However, this is not always straightforward. Many programming languages exhibit
undefined behavior, in which a particular syntactic construct has an unknown
effect. For example, dereferencing an uninitiated pointer is undefined, and the
compiler may take advantage of this and do whatever it wants. Typically
``whatever it wants'' is something reasonable, but it is not guaranteed to be
useful or consistent.

The following statements in C may print anything, as it is undefined behavior:\\
\texttt{int *p;\\ printf("\%d\char`\\ n", *p);}

There are several classic optimizations:

\begin{itemize}
  \item Strength Reduction: replace an expensive operation with a cheaper one that is semantically identical.\\
    Example: Division may be replaced by bit shifts. $8/2 \equiv 8 >> 1$
   \item Dead Value Elimination: a dead value is a value that does not affect the
    result of a function; these can be eliminated without changing the
    function. Dead values are discovered using dependency analysis, which
    creates an acyclic directed graph of all the statements in the program; any
    statements outside of the graph can be eliminated. For example:
    \begin{verbatim}
    inf fun(int v)            Graph: Return
        int x = 2;                     |
        int y = v + 1                  y
        return y                      / \
                                     v   1 
    \end{verbatim}
    The variable $x$ may be safely eliminated because it is not in the graph.
  \item Constant Propagation: the values of known constants are substituted into
    expressions at compile time. These substitutions can enable further
    optimizations. For example, a function f(x) that returns $x+1$ can be
    replaced with $x+1$ if the value of $x$ is known.
  \item Function Inlining: small function calls are replaced with their body,
    which eliminates the overhead (stack allocation, jumps, post-function
    cleanup, etc.) of the function call.
  \item Register Allocation: controlling which values are assigned to registers
    can reduce load and store calls. This optimization can be reduced to a graph
    coloring problem; the variables in the program are the vertexes, which are
    connected by which variables need to be used at what times.
  \item Instruction Selection: different instructions may have the same result,
    but they may not take the same amount of time. For example, zeroing
    register \texttt{AX} may be accomplished with \texttt{MOVE 0,AX}, but
    \texttt{XOR~AX,AX} does the same thing more efficiently.
  \item Tail Call Optimization: if the last statement of a function is a call to
    itself, the call can essentially be replaced with a GOTO to the beginning of
    the function. A new stack frame does not need to be allocated, so memory is
    preserved.
\end{itemize}

\section{LISP}

Lisp (LISt Processing) has many descendents, such as Scheme, Clojure and Racket.
It was originally intended for ``Classic'' AI, which had the goal of creating
intelligence by programming a logical system and describing states and the
transformations upon them. (This does not affect their expressive or
computational power, as they are Turing complete, as most programming languages
are.)

Fortran used simple types, like ints and floats, with no way to express
structures. Lisp, on the other hand, focuses on expressiveness and structure
with few types. The language is based around mathematics. Managing memory
(allocation, deallocation, etc.) is not particularly mathematical, so Lisp
doesn't do it either. Instead, automatic memory management handles memory
transactions. (This is in contrast to languages with explicit or manual memory
management.)

Example Lisp function:
\begin{verbatim}
(defun time ()
    (prog (output(timeval))
          (time)))
\end{verbatim}
This function outputs the current time (\texttt{timeval}) and then recurses.
Because Lisp does not have tail call optimization, this program will eventually
run out of stack space. Each call to \texttt{time} (and \texttt{timeval}) will
create another stack frame, which will never be cleaned up because the function
does not return.

\section{Tangents}

\subsection{History of the Term `Computer'}
``Computer'' originally referred to a person who performed calculations. For
example, computers were used to decrypt transmissions during World War II. Many
early developments in computer science were for ``automatic computing'' --
calculations performed mechanically or electronically.

\subsection{Brief Overview of Typing}
There are three kinds of typing: untyped, static and dynamic.
A type describes and confines the values assignable to a variable of that type.
Untyped variables are essentially containers that can hold anything. With static
typing, you assign types to boxes, and those boxes can only values of that type.
With dynamic typing, values have types but boxes do not.

\subsection{JavaScript and R}
JavaScript is essentially the untyped lambda calculus. Which is impressive for a
language designed in only 10 days.

R is a dynamically-typed language designed for statistical computing. It has
many type coercions that can be confusing. For example:
\texttt{[3,4,7]~+~1 $\equiv$ [3,5,7]~+~1 $\equiv$ [3,5,7]~+~[1,1,1] = [4,6.8]}
However, \texttt{[3,5,7,9,11]~+~[2,4] $\equiv$ [3,5,7,9,11]~+~[2,4,2,4,2]}. This
is allegedly useful for statistical computations.

\end{document}
