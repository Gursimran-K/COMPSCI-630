\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\usepackage{graphicx}
\usepackage{url}

%
% The following commands sets up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}
\newcommand{\dnl}{\mbox{}\par}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
  \pagestyle{myheadings}
  \thispagestyle{plain}
  \newpage
  \setcounter{lecnum}{#1}
  \setcounter{page}{1}
  \noindent
  \begin{center}
  \framebox{
     \vbox{\vspace{2mm}
   \hbox to 6.28in { {\bf CMPSCI~630~~~Systems
                       \hfill Fall 2019} }
      \vspace{4mm}
      \hbox to 6.28in { {\Large \hfill Lecture #1  \hfill} }
%       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
      \vspace{2mm}
      \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
     \vspace{2mm}}
  }
  \end{center}
  \markboth{Lecture #1: #2}{Lecture #1: #2}
  \vspace*{4mm}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
%
\renewcommand{\cite}[1]{[#1]}

% \input{epsf}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{FIGURE-SIZE}{CAPTION}{FILENAME}
\newcommand{\fig}[4]{
           \vspace{0.2 in}
           \setlength{\epsfxsize}{#2}
           \centerline{\epsfbox{#4}}
           \begin{center}
           Figure \thelecnum.#1:~#3
           \end{center}
   }

% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% Some useful equation alignment commands, borrowed from TeX
\makeatletter
\def\eqalign#1{\,\vcenter{\openup\jot\m@th
 \ialign{\strut\hfil$\displaystyle{##}$&$\displaystyle{{}##}$\hfil
     \crcr#1\crcr}}\,}
\def\eqalignno#1{\displ@y \tabskip\@centering
 \halign to\displaywidth{\hfil$\displaystyle{##}$\tabskip\z@skip
   &$\displaystyle{{}##}$\hfil\tabskip\@centering
   &\llap{$##$}\tabskip\z@skip\crcr
   #1\crcr}}
\def\leqalignno#1{\displ@y \tabskip\@centering
 \halign to\displaywidth{\hfil$\displaystyle{##}$\tabskip\z@skip
   &$\displaystyle{{}##}$\hfil\tabskip\@centering
   &\kern-\displaywidth\rlap{$##$}\tabskip\displaywidth\crcr
   #1\crcr}}
\makeatother

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:



% Some general latex examples and examples making use of the
% macros follow.

\begin{document}

%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{19}{December 3}{Emery Berger}{Deepali Garg}

\section{Profilers}

The class began with a conclusion from the previous class which covered profiling. 

\textbf{Types of profiling}

Instrumentation based - have a big probe effect, expensive, slow things down.

Sampling-based - like gProf which stop program periodically to find hotspots in program.

Performance counter-based - to find what’s going on like branch prediction failures etc.

Causal profiling - experiment based, give actual causality, different from correlation of other programs.

They address problems at a single node. What about distributed systems? One of the challenges with distributed systems in a shared clock. There is currently no synchronized clock. If there was one, everything becomes trivial. E.g. Fischer lynch Paterson’s result- it’s impossible to have an agreement even on the value of one bit in a dist system. The consensus is impossible. Time is a huge problem. Drift happens even when clocks begin at the same time.

Network Time Protocol has known time servers. It helps maintain synchronization between servers. Another type is the Atomic Clock which is a perfect timekeeper. It has a radioactive substance that decays at a fixed rate and it is based on it. It provides a quasi-synch time. Google uses it, considering events happening in the same window as occurring at the same time. 

A Lamport clock helps to maintain global order. 

But time isn’t the only problem. How do we get performance information? How do we handle network delays, caching, etc? There’s a lot of variabilities. e.g. Memcached is used in Facebook for caching. The PHP API call resolution in the backend is intensive, so we like to keep the data in the cache. 

Profiling dist systems is an open question because of these reasons. 

Profiling mixed languages is complex. In JS compiler generates code. The problem is mapping compiled code with source code. JIT-compiler optimizes code over and over which makes it difficult. It moves from slow(snail) to fast(rabbit) territory. Python runs interpreters, C runs libraries. We don’t want to go inside them and asking them to speed things up.


\section{Correctness}

Correctness is hard. There’s no randomness in the occurrence of bugs later. It is monotonically increasing and compositional. If there’s a bug or there’s correctness, it accumulates and builds. 

The program refines a specification which can lead to a subset of different programs. The challenge is that specifying full correctness is very hard for programs. Partial correctness is with respect to some part of the program.  E.g. a property that it doesn’t dereference NULL. A bug checker goes to find if a program has a bug. an always error is something like dereferencing null or memory safety error. 

Full correctness is called verification. It has been applied to domains like security and very narrow API where the specification is very simple. These domains are important and have small specs. E.g. device drivers in operating systems. Microkernels that have been verified. L4 in embedded systems. 

OpenSSL is used for establishing secure connections. It has not been verified. It was vulnerable to the heartbleed bug. Microsoft embarked on the Everest project that wrote a formally verified implementation of OpenSSL.  They wrote it in F* that compiles to C and runs it. Building high assurance software is expensive and thus only applies to things that really matter. 

Correctness does not imply that cryptographic measures are correct. Testing can prove the presence of bugs but not their absence. The state space is so large that only after exhaustive testing you can prove correctness. 

\section{Verification}

Verification is hard. The specification is something that is infinite and it is correct with respect to some other specification.  

For most software, we can’t verify things, so we do dynamic analysis, static analysis, and testing. The difference is that dynamic is when the program runs, static executes program symbolically. Dynamic is more precise with low false positives. It has a low recall, so it can miss out on bugs that are not exposed during execution. Static analysis is imprecise with high false positives since it works on approximations. 

Sound - if there’s any possibility of an error, report it. E.g. potential divide by zero occurrences is reported in soundness. Dynamic analysis already does this. In theory, you can’t test everything so static analysis is way better. But false positives are a killer. 

Coverity is a static analysis tool company. It has a free service but its not so good. 

Heroic static analysis is when you throw everything at it. 

How can we reduce false positives yet maintain soundness? It is a big research issue right now.  

Dynamic analysis sounds bad but they tell you exactly where the problem is. There’s a sanitizer by Google where you can plug in different things, it can be ASan, TSan, etc. It will insert checks for every single reference. 

There’s a tool from Intel called Pin that does dynamic analysis. Valgrind is the epitome of the heavyweight way of doing this. But it is very slow and is mostly used to check memory safety. ASan is a bit faster. 

GC is a kind of dynamic analysis but it eliminates errors instead of reporting them. The Exterminator system detects memory errors and fixes the program. It injects something into the runtime system. 


% DieHard tries to provide something along these lines, within the constraints
% of finite physical memory. It uses randomized heap allocation, so objects
% are not necessarily contiguous in virtual memory. Since the address space
% is actually finite, objects won't actually be infinitely far apart, and
% buffer overruns might actually cause collisions between heap objects. But this
% is where the multicore processors come in: With the unused processor cores,
% run multiple copies of the application, say three copies,
% each allocating into their own randomized heap. So the heap errors are
% independent among the three copies of the application.
% All copies get the same input,
% and the output is the result of voting among the three copies of the program.
% If one instance of the application disagrees with the other two, it is killed,
% since there was likely a collision between heap objects in that one. Similarly,
% if one instance dies with a segfault or other error, the others remain running.
% Surviving copies can be forked to replace copies which were killed off, though
% this reduces the independence among copies.

% This is transparent to correct applications, and allows erroneous applications
% to survive longer, which will make their users happier. Furthermore, when
% a copy dies or is killed off, the error can be noted and sent to the software
% maintainers automatically.

% DieHard increases the chances of turning problematic errors into
% benign errors. It trades memory space for robustness, which is probably
% fine as memory prices continue to drop. It uses randomization and
% replication to provide fault-tolerance.

\end{document}